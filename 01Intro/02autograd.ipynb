{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7ev9xtnDU0G2",
    "school_cell_uuid": "0be508e1695e4b329b723279d00061ed"
   },
   "source": [
    "## 자동 미분, autograd "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mx2ij_jgU0G3",
    "school_cell_uuid": "49fe360451c843d1b115b31818e1165b"
   },
   "source": [
    "``autograd`` 패키지는 Tensor의 모든 연산에 대해 자동 미분을 제공한다.\n",
    "이는 실행-기반-정의(define-by-run) 프레임워크로, 이는 코드를 어떻게 작성하여 실행하느냐에 따라 역전파가 정의된다는 뜻이며, 역전파는 학습 과정의 매 단계마다 달라진다.  \n",
    "\n",
    "* autograd는 PyTorch에서 핵심적인 기능을 담당하는 하부 패키지이다.  \n",
    "* autograd는 텐서의 연산에 대해 자동으로 미분값을 구해주는 기능을 한다.\n",
    "* 텐서 자료를 생성할 때, `requires_grad`인수를 `True`로 설정하거나 `.requires_grad_(True)`를 실행하면 그 텐서에 행해지는 모든 연산에 대한 미분값을 계산한다.   \n",
    "* 계산을 멈추고 싶으면 `.detach()`함수를 이용하면 된다. \n",
    "\n",
    "\n",
    "예제를 통해 알아 보도록 하자. `requires_grad`인수를 `True`로 설정하여  Tensor를 생성했다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "Rjj9cX1NU0G3",
    "outputId": "65cf3c95-f43d-4f53-ba06-da900fcf9574",
    "school_cell_uuid": "3396443a49494a22afb7fb0c7801aaff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9863, 0.9045],\n",
      "        [0.5759, 0.9950]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(2, 2, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pe3UmFHEU0G6",
    "school_cell_uuid": "bbe2ffda94494f22aff74255a81349f0"
   },
   "source": [
    "다음으로 이 x에 연산을 수행한다. 다음 코드의 y는 연산의 결과이므로 미분 함수를 가진다. `grad_fn`속성을 출력해 미분 함수를 확인 할 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "DbM8mcngU0G7",
    "outputId": "cedad3f8-ae83-44ed-b0f6-7537c1006cfb",
    "school_cell_uuid": "d3fd2bc942a94648b083b9f6d22887b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.3852, grad_fn=<SumBackward0>) <SumBackward0 object at 0x0000017472511040>\n"
     ]
    }
   ],
   "source": [
    "y = torch.sum(x * 3)\n",
    "print(y, y.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tHMz6Oz0U0G9",
    "school_cell_uuid": "85d36e8f00a4460abff07fb15125a86e"
   },
   "source": [
    "`y.backward()` 함수를 실행하면 x의 미분값이 자동으로 갱신된다. x의 `grad`속성을 확인하면 미분값이 들어 있는 것을 확인 할 수 있다. y를 구하기 위한 x의 연산을 수식으로 쓰면 다음과 같다. \n",
    "\n",
    "$$\n",
    "y = \\displaystyle\\sum_{i=1}^4 3 \\times x_i\n",
    "$$\n",
    "\n",
    "이를 $x_i$에 대해 미분 하면 미분 함수는 다음과 같다. \n",
    "\n",
    "$$\n",
    "\\dfrac{\\partial y}{\\partial x_i} = 3\n",
    "$$\n",
    "\n",
    "실제 미분값과 같은지 확인해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "OEeR1Oh4U0G-",
    "outputId": "1d864805-1c33-4832-9bee-888ea2028820",
    "school_cell_uuid": "f345c4eb6f7b4cda9ef4ce286e1a62c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3., 3.],\n",
       "        [3., 3.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.backward() # gradient를 계산한다.\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[7., 7.],\n",
       "        [7., 7.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y2 = torch.sum(x*4)\n",
    "# y2.backward()\n",
    "# x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4., 4.],\n",
       "        [4., 4.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y2 = torch.sum(x*4)\n",
    "# x.grad.zero_()\n",
    "# y2.backward()\n",
    "# x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### retain_graph \n",
    "\n",
    "- retain_graph = False가 기본이다.\n",
    "- retain_graph = True ratain 이라는 말에서 알 수 있듯이 텐서들의 연속된 연산으로 구성되는 graph 를 유지한다. \n",
    "- retain_graph = False 이면 두 번째 backward를 시행시 저장되어 있던 중간 결과값들이 free 되었다고 나오면서 retain_graph=True 로 설정하라고 나온다. \n",
    "- Pytorch 에서는 backward() 메소드를 한 번 수행하면 기울기가 계산되면서 이를 계산하기 위한 중간 결과값들이 없어진다. 따라서 backward를 반복적으로 수행할 경우 retain_graph=True 로 수행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-47895a788594>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 2번째 backward() 호출시 retain_graph=False 로 에러가 난다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             )\n\u001b[1;32m--> 492\u001b[1;33m         torch.autograd.backward(\n\u001b[0m\u001b[0;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    494\u001b[0m         )\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    249\u001b[0m     \u001b[1;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m     \u001b[1;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 251\u001b[1;33m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "y.backward() # 2번째 backward() 호출시 retain_graph=False 로 에러가 난다.\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U65HX9vEU0HB",
    "school_cell_uuid": "52d6d6b52cf14a23b4cd076f7b0963d1"
   },
   "outputs": [],
   "source": [
    "x = torch.rand(2, 2, requires_grad=True)\n",
    "y = torch.sum(x * 3)\n",
    "\n",
    "y.backward(retain_graph=True) # retain_graph = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "6PmT-yBFU0HE",
    "outputId": "cb7b9df0-3eaa-4d12-de23-c1d1cfb47309",
    "school_cell_uuid": "bfcfdfea33f843fbabb7c58305594b1f",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3., 3.],\n",
       "        [3., 3.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6., 6.],\n",
       "        [6., 6.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.backward(retain_graph=True) # grad는 누적된다.\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J_jJvUmpU0HH",
    "school_cell_uuid": "912bc4592afa4dc3a7ae47a96278601a"
   },
   "source": [
    "* ``.backward()`` 를 호출하여 모든 변화도(gradient)를 자동으로 계산할 수 있다.  \n",
    "* 모든 연산 과정을 encode 하여 순환하지 않는 그래프(acyclic graph)를 생성한다.  \n",
    "* 각 tensor는 ``.grad_fn`` 속성을 갖고 있는데, 이는 ``Tensor`` 를 생성한 ``Function`` 을 참조하고 있다. \n",
    "\n",
    "* `backward()`함수는 자동으로 미분값을 계산해 `requires_grad`인수가 True로 설정된 변수의 `grad`속성의 값을 갱신한다.\n",
    "* 미분값을 그대로 출력받아 사용하고 싶은 경우에는 `torch.autograd.grad()`함수에 출력값과 입력값을 입력하면 미분값을 출력한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "RRdIQtotU0HH",
    "outputId": "53565a86-fe43-4e27-ac58-95bf257368c2",
    "school_cell_uuid": "39d3203d06944519aa6b7c16e98aeda6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[3., 3.],\n",
       "         [3., 3.]]),)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.grad(y, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5aFWG_EnU0HK",
    "school_cell_uuid": "a075cd7db9344c589ce6018801d4b519"
   },
   "source": [
    "상황에 따라 특정 연산에는 미분값을 계산하고 싶지 않은 경우에는 `.detach()`함수를 사용한다. 예를 들어, 이전 코드의 결과 값 `y`에 로지스틱 함수 연산을 수행하고 이에 대한 미분 값을 계산 하고 싶지 않은 경우에 다음처럼 할 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "school_cell_uuid": "cc57a8f14f6846e2b70a1c276d3e408b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9508)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_1 = y.detach()\n",
    "torch.sigmoid(y_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "또한 ``with torch.no_grad():`` 로 코드 블럭을 감싸서 autograd가  \n",
    "``.requires_grad=True`` 인 Tensor들의 연산 기록을 추적하는 것을 멈출 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "\n",
    "print(x.requires_grad)\n",
    "print((x ** 2).requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print((x ** 2).requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2차 미분 수행하기, second derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(1., requires_grad = True)\n",
    "y = 2*x**3 + 5*x**2 + 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_derivative = torch.autograd.grad(y, x, \n",
    "                                       retain_graph=True, \n",
    "                                       create_graph=True)\n",
    "second_derivative = torch. autograd.grad(first_derivative, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(22.),)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autograd 연습\n",
    "\n",
    "autograd는 텐서의 연산에 대해 자동으로 미분값을 구해주는 기능을 한다. 텐서 자료를 생성할 때, requires_grad인수를 True로 설정하거나 .requires_grad_(True)로 설정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 다음과 같은 텐서를 생성하시오.\n",
    "```\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = 9*x**4 + 2*x**3 + 3*x**2 + 6*x +1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = 9*x**4 + 2*x**3 + 3*x**2 + 6*x +1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. y를 x에 대해 미분하고 그 값을 출력해 보시오. .backward()를 사용하면 자동으로 미분값을 계산한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(330., grad_fn=<AddBackward0>),)\n"
     ]
    }
   ],
   "source": [
    "first_derivative = torch.autograd.grad(y, x, \n",
    "                                       retain_graph=True, \n",
    "                                       create_graph=True)\n",
    "print(first_derivative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(330.)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 다음과 같은 텐서를 생성하시오.\n",
    "```\n",
    "x = torch.tensor(1.0, requires_grad=True)\n",
    "z = torch.tensor(2.0, requires_grad=True)\n",
    "y = x**2 + z**3\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(1.0, requires_grad=True)\n",
    "z = torch.tensor(2.0, requires_grad=True)\n",
    "y = x**2 + z**3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 3번에서 생성한 텐서에서 y를 x에 대해 편미분한 값과 y를 z에 대해 편미분한 값을 출력하시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(2., grad_fn=<MulBackward0>),)\n",
      "(tensor(12., grad_fn=<MulBackward0>),)\n"
     ]
    }
   ],
   "source": [
    "first_derivative_x = torch.autograd.grad(y, x, \n",
    "                                       retain_graph=True, \n",
    "                                       create_graph=True)\n",
    "print(first_derivative_x)\n",
    "\n",
    "first_derivative_z = torch.autograd.grad(y, z, \n",
    "                                       retain_graph=True, \n",
    "                                       create_graph=True)\n",
    "print(first_derivative_z)\n",
    "                                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. 3번에서 생성한 x의 값을 출력해 보시오. .data를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
